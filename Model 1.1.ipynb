{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import traceback\n",
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout, Lambda, LeakyReLU, AveragePooling2D, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.pyplot import hist\n",
    "from PIL import Image\n",
    "from scipy.stats import zscore\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config & Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_WIDTH, IMAGE_HEIGHT = 227, 227\n",
    "random.seed(52)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(input_data, output_data, batch_size, is_training):\n",
    "    input_data_batch = np.empty([batch_size, IMAGE_HEIGHT, IMAGE_WIDTH, 1])\n",
    "    output_data_batch = np.empty([batch_size, 1])\n",
    "    while True:\n",
    "        index = 0\n",
    "        for i in np.random.permutation(input_data.shape[0]):\n",
    "            input_data_batch[index] = input_data[i]\n",
    "            output_data_batch[index] = output_data[i]\n",
    "            index += 1\n",
    "            if index == batch_size:\n",
    "                break\n",
    "        yield input_data_batch, output_data_batch\n",
    "\n",
    "\n",
    "def _preprocess(image_loc: str) -> np.ndarray:\n",
    "    f_image = Image.open(image_loc).convert('L')\n",
    "    f_image = f_image.resize((IMAGE_WIDTH, IMAGE_HEIGHT), Image.ANTIALIAS)\n",
    "    image = np.asarray(f_image)\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    image = np.expand_dims(image, axis=3)\n",
    "    return image\n",
    "\n",
    "\n",
    "def preprocess(loc: str) -> np.ndarray:\n",
    "    \"\"\"Safety wrapper over _preprocess.\n",
    "    \n",
    "    Args:\n",
    "        loc: File location to check if dir or an image.\n",
    "        \n",
    "    Returns:\n",
    "        Preprocessed image(s) as an ndarray.\n",
    "    \"\"\"\n",
    "    if os.path.isdir(loc):\n",
    "        current = np.concatenate((\n",
    "            [_preprocess(os.path.join(loc, image)) for image in os.listdir(loc)\n",
    "             if '.png' in image]\n",
    "        ))\n",
    "    else:\n",
    "        current = _preprocess(loc)\n",
    "    return current\n",
    "\n",
    "\n",
    "def generate_data_arrays(\n",
    "    covid_data: str = 'rsc/New_Data_CoV2/COVID/',\n",
    "    noncovid_data: str = 'rsc/New_Data_CoV2/non-COVID/',\n",
    "    others_data:  str = 'rsc/New_Data_CoV2/Others/',\n",
    "    image_output: str = 'rsc/input_data_v2',\n",
    "    label_output: str = 'rsc/output_data_v2',\n",
    "    regenerate: bool = True\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Loads COVID images into ndarrays.\n",
    "    \n",
    "    Args:\n",
    "        covid_data: Relative path to folder containing COVID images.\n",
    "        noncovid_data: Relative path to folder containing non-COVID images.\n",
    "        others_data: Relative path to folder containing Others images.\n",
    "        image_output: Relative path (with filename) to save loaded images to.\n",
    "        label_output: Relative path (with filename) to save loaded classes (labels) to.\n",
    "        regenerate: If True, will regenerate saved ndarray files for both image_output\n",
    "            and label_output, even if they already exist (overwriting them). If False,\n",
    "            will simply load and return the aforementioned.\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of ndarrays. The first is the loaded image data in one large ndarray,\n",
    "        and the second is the loaded labels, also in a large ndarray. These directly\n",
    "        match up to each other ONLY when both arrays (and their respective files)\n",
    "        are generated at the same time with this method.\n",
    "    \"\"\"\n",
    "    if (regenerate is False and \n",
    "        os.path.isfile(f'{out_input_data}.npy') and\n",
    "        os.path.isfile(f'{out_output_data}.npy')):\n",
    "        return (np.load(f'{out_input_data}.npy'),\n",
    "                np.load(f'{out_output_data}.npy'))\n",
    "    \n",
    "    if not ((cd := os.path.exists(covid_data)) and (ncd := os.path.exists(noncovid_data))):\n",
    "        if (others_data is not None) and not (odd := os.path.exists(others_data)):\n",
    "            raise NotADirectoryError(\"Directories do not exist.\")\n",
    "        else:\n",
    "            raise NotADirectoryError(\"COVID/non-COVID directories do not exist.\")\n",
    "    \n",
    "    im_data = np.empty([1, IMAGE_WIDTH, IMAGE_WIDTH, 1])\n",
    "    output = np.array([0])\n",
    "\n",
    "    print('Loading Covid Data')\n",
    "    for i in tqdm(glob.glob(os.path.join(covid_data, '**'))):\n",
    "        current = preprocess(i)\n",
    "        im_data = np.concatenate((im_data, current))\n",
    "        output = np.concatenate((output, np.array([1])), axis=0)\n",
    "\n",
    "    print('Loading non-Covid Data')\n",
    "    for i in tqdm(glob.glob(os.path.join(noncovid_data, '**'))):\n",
    "        current = preprocess(i)\n",
    "        im_data = np.concatenate((im_data, current))\n",
    "        output = np.concatenate((output, np.array([0])), axis=0)\n",
    "        \n",
    "    if others_data is not None:\n",
    "        print('Loading Others Data')\n",
    "        for i in tqdm(glob.glob(os.path.join(others_data, '**'))):\n",
    "            current = preprocess(i)\n",
    "            im_data = np.concatenate((im_data, current))\n",
    "            output = np.concatenate((output, np.array([-1])), axis=0)\n",
    "    else:\n",
    "        print('Others data not provided; skipping.')\n",
    "\n",
    "\n",
    "    print('Finished Loading Data')\n",
    "    im_data = np.delete(im_data, 0, axis=0)\n",
    "    output = np.delete(output, 0, axis=0)\n",
    "\n",
    "    zip_data = list(zip(im_data, output))\n",
    "    random.shuffle(zip_data)\n",
    "    im_data, output = zip(*zip_data)\n",
    "\n",
    "    np.save(f'{image_output}', im_data)\n",
    "    np.save(f'{label_output}', output)\n",
    "    return im_data, output\n",
    "\n",
    "\n",
    "def load_data(image_loc: str, label_loc: str, **kwargs) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Attempts to load generated images and labels.\n",
    "    \n",
    "    If either is not found, attempts to (re)generate them using the default\n",
    "    input arguments to the generation method for the raw data, but the input\n",
    "    `image_loc` and `label_loc` parameters for the output arrays. If this\n",
    "    still does not work, returns a general error and exits.\n",
    "    \n",
    "    Args:\n",
    "        image_loc: Relative path to saved image ndarray file.\n",
    "        label_loc: Relative path to saved label ndarray file.\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of ndarrays. The first array holds the loaded image data, whilst\n",
    "        the second array holds their respective labels.\n",
    "    \n",
    "    \"\"\"\n",
    "    try:\n",
    "        image_data = np.load(f'{image_loc}.npy')\n",
    "        label_data = np.load(f'{label_loc}.npy')\n",
    "        return image_data, label_data\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        try:\n",
    "            image_data, label_data = generate_data_arrays(\n",
    "                image_output = image_loc,\n",
    "                label_output = label_loc,\n",
    "                **kwargs\n",
    "            )\n",
    "            return image_data, label_data\n",
    "        \n",
    "        except Exception as e:\n",
    "            traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Version 1\n",
    "im_data_v1, output_v1 = load_data(\n",
    "    image_loc = 'rsc/input_data_v1',\n",
    "    label_loc = 'rsc/output_data_v1',\n",
    "    covid_data = 'rsc/Data_CoV2/COVID/',\n",
    "    noncovid_data = '/rsc/Data_CoV2/non-COVID/',\n",
    "    others_data = None,\n",
    ")\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(im_data_v1, output_v1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(X_Train, Y_Train, X_Test, Y_Test, model_dir: str):\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "    #X_Train, X_Test, Y_Train, Y_Test = train_test_split(input_data, output_data, test_size=0.1)\n",
    "\n",
    "    for_pad = lambda s: s if s > 2 else 3\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Lambda(lambda x: x, input_shape=(IMAGE_WIDTH, IMAGE_HEIGHT, 1)))\n",
    "\n",
    "    #model.add(ZeroPadding2D(padding=(for_pad(3) - 1)//2))\n",
    "    model.add(Conv2D(kernel_size=(3, 3), filters=8, strides=(1, 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid'))\n",
    "\n",
    "    #model.add(ZeroPadding2D(padding=(for_pad(3) - 1)//2))\n",
    "    model.add(Conv2D(kernel_size=(3, 3), filters=16, strides=(1, 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "    #model.add(ZeroPadding2D(padding=(for_pad(3) - 1)//2))\n",
    "    model.add(Conv2D(kernel_size=(3, 3), filters=32, strides=(1, 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    #model.add(ZeroPadding2D(padding=(for_pad(1) - 1)//2))\n",
    "    model.add(Conv2D(kernel_size=(1, 1), filters=16, strides=(1, 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    #model.add(ZeroPadding2D(padding=(for_pad(3) - 1)//2))\n",
    "    model.add(Conv2D(kernel_size=(3, 3), filters=32, strides=(1, 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "    #model.add(ZeroPadding2D(padding=(for_pad(3) - 1)//2))\n",
    "    model.add(Conv2D(kernel_size=(3, 3), filters=64, strides=(1, 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    #model.add(ZeroPadding2D(padding=(for_pad(1) - 1)//2))\n",
    "    model.add(Conv2D(kernel_size=(1, 1), filters=32, strides=(1, 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    #model.add(ZeroPadding2D(padding=(for_pad(3) - 1)//2))\n",
    "    model.add(Conv2D(kernel_size=(3, 3), filters=64, strides=(1, 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "\n",
    "    #model.add(ZeroPadding2D(padding=(for_pad(3) - 1)//2))\n",
    "    model.add(Conv2D(kernel_size=(3, 3), filters=128, strides=(1, 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    #model.add(ZeroPadding2D(padding=(for_pad(1) - 1)//2))\n",
    "    model.add(Conv2D(kernel_size=(1, 1), filters=64, strides=(1, 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    #model.add(ZeroPadding2D(padding=(for_pad(3) - 1)//2))\n",
    "    model.add(Conv2D(kernel_size=(3, 3), filters=128, strides=(1, 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid'))\n",
    "\n",
    "\n",
    "    #model.add(ZeroPadding2D(padding=(for_pad(3) - 1)//2))\n",
    "    model.add(Conv2D(kernel_size=(3, 3), filters=256, strides=(1, 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    #model.add(ZeroPadding2D(padding=(for_pad(1) - 1)//2))\n",
    "    model.add(Conv2D(kernel_size=(1, 1), filters=128, strides=(1, 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "    model.add(AveragePooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid'))\n",
    "\n",
    "    #model.add(ZeroPadding2D(padding=(for_pad(3) - 1)//2))\n",
    "    model.add(Conv2D(kernel_size=(3, 3), filters=256, strides=(1, 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "    model.add(AveragePooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid'))\n",
    "    #model.add(ZeroPadding2D(padding=(for_pad(1) - 1)//2))\n",
    "    model.add(Conv2D(kernel_size=(1, 1), filters=128, strides=(1, 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "\n",
    "\n",
    "    #model.add(ZeroPadding2D(padding=(for_pad(19) - 1)//2))\n",
    "    model.add(Conv2D(kernel_size=(3, 3), filters=256, strides=(1, 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "    #model.add(Conv2D(kernel_size=(3, 3), filters=2, strides=(1, 1),activation='relu'))\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dropout(rate=0.4))\n",
    "    model.add(Dense(units=338, activation='relu'))\n",
    "    model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(optimizer=Adam(lr=3e-4), loss='binary_crossentropy', metrics=[\"accuracy\"])\n",
    "    model.summary()\n",
    "\n",
    "    checkpoint = ModelCheckpoint(os.path.join(model_dir, 'xRGM1.1'), verbose=1, period=1, save_best_only=True)\n",
    "\n",
    "    hist = model.fit_generator(batch_generator(X_Train, Y_Train, 64, True),\n",
    "                        validation_data=(batch_generator(X_Test, Y_Test, 64, False)), callbacks=[checkpoint],\n",
    "                        verbose=1, epochs=6, steps_per_epoch=1000, validation_steps=len(X_Test))\n",
    "    with open(os.path.join('models', 'xRGM1.1-Net-HistDict'), 'wb') as f:\n",
    "        pickle.dump(hist.history, f)\n",
    "\n",
    "    model.save(model_dir)\n",
    "    \n",
    "    return model\n",
    "#changed epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lambda (Lambda)              (None, 227, 227, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 225, 225, 8)       80        \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 225, 225, 8)       32        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 225, 225, 8)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 112, 112, 8)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 110, 110, 16)      1168      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 110, 110, 16)      64        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 110, 110, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 108, 108, 32)      4640      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 108, 108, 32)      128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 108, 108, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 108, 108, 16)      528       \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 108, 108, 16)      64        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 108, 108, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 106, 106, 32)      4640      \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 106, 106, 32)      128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 106, 106, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 104, 104, 64)      18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 104, 104, 64)      256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 104, 104, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 104, 104, 32)      2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 104, 104, 32)      128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 104, 104, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 102, 102, 64)      18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 102, 102, 64)      256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 102, 102, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 100, 100, 128)     73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 100, 100, 128)     512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 100, 100, 128)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 100, 100, 64)      8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 100, 100, 64)      256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 100, 100, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 98, 98, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 98, 98, 128)       512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 98, 98, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 49, 49, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 47, 47, 256)       295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 47, 47, 256)       1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 47, 47, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 47, 47, 128)       32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 47, 47, 128)       512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)   (None, 47, 47, 128)       0         \n",
      "_________________________________________________________________\n",
      "average_pooling2d (AveragePo (None, 23, 23, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 21, 21, 256)       295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 21, 21, 256)       1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)   (None, 21, 21, 256)       0         \n",
      "_________________________________________________________________\n",
      "average_pooling2d_1 (Average (None, 10, 10, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 10, 10, 128)       32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 10, 10, 128)       512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)   (None, 10, 10, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 8, 8, 256)         295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 8, 8, 256)         1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)   (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 16384)             0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 16384)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 338)               5538130   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 339       \n",
      "=================================================================\n",
      "Total params: 6,702,293\n",
      "Trainable params: 6,699,077\n",
      "Non-trainable params: 3,216\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Epoch 1/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jg1994/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 53190s 53s/step - loss: 0.1358 - accuracy: 0.9439 - val_loss: 0.1022 - val_accuracy: 0.9706\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.10224, saving model to model/xRGM1.1/xRGM1.1\n",
      "INFO:tensorflow:Assets written to: model/xRGM1.1/xRGM1.1/assets\n",
      "Epoch 2/6\n",
      "1000/1000 [==============================] - 62476s 62s/step - loss: 8.0715e-05 - accuracy: 1.0000 - val_loss: 0.0796 - val_accuracy: 0.9782\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.10224 to 0.07957, saving model to model/xRGM1.1/xRGM1.1\n",
      "INFO:tensorflow:Assets written to: model/xRGM1.1/xRGM1.1/assets\n",
      "Epoch 3/6\n",
      "1000/1000 [==============================] - 62129s 62s/step - loss: 1.2217e-05 - accuracy: 1.0000 - val_loss: 0.0840 - val_accuracy: 0.9776\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.07957\n",
      "Epoch 4/6\n",
      "1000/1000 [==============================] - 61756s 62s/step - loss: 6.9683e-06 - accuracy: 1.0000 - val_loss: 0.0791 - val_accuracy: 0.9822\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.07957 to 0.07909, saving model to model/xRGM1.1/xRGM1.1\n",
      "INFO:tensorflow:Assets written to: model/xRGM1.1/xRGM1.1/assets\n",
      "Epoch 5/6\n",
      " 355/1000 [=========>....................] - ETA: 9:51:46 - loss: 3.0553e-06 - accuracy: 1.0000"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_dir: str = 'model/xRGM1.1'\n",
    "xRGM = make_model(x_train, y_train.reshape(-1,1), x_test, y_test.reshape(-1,1), model_dir=model_dir)\n",
    "\n",
    "v_classify = np.vectorize(lambda x: np.round(x, 0))\n",
    "# xRGMCT2 = load_model('models/xRGMCT2L.h5')\n",
    "# print(xRGMCT2.history)\n",
    "y_pred = (xRGM.predict(x_test))\n",
    "\n",
    "#print(y_test)\n",
    "#print(y_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = pd.DataFrame(metrics.confusion_matrix(y_test, v_classify(y_pred)),\n",
    "                                columns=['Predicted Negative', 'Predicted Positive'],\n",
    "                                index=['Actual Negative', 'Actual Positive'])\n",
    "\n",
    "tp, tn, fp, fn = (\n",
    "    confusion_matrix.iloc[1,1], \n",
    "    confusion_matrix.iloc[0,0], \n",
    "    confusion_matrix.iloc[0,1], \n",
    "    confusion_matrix.iloc[1,0]\n",
    ")\n",
    "\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "sensitivity = tp / (tp + fn)\n",
    "specificity = tn / (tn + fp)\n",
    "f1_score = 2 * ((specificity * sensitivity) / (specificity + sensitivity))\n",
    "\n",
    "with open(os.path.join(f'{model_dir}', 'metrics.csv'), 'a', encoding='utf-8') as f:\n",
    "    f.write(\n",
    "        f'{time.strftime(\"%Y-%m-%dT%H:%M:%S-05:00\", time.localtime())}, {accuracy}, {sensitivity}, {specificity}, {f1_score}\\n'\n",
    "    )\n",
    "\n",
    "print(f'Accuracy:    {accuracy}')\n",
    "print(f'Sensitivity: {sensitivity}')\n",
    "print(f'Specificity: {specificity}')\n",
    "print(f'F1 Score:    {f1_score}')\n",
    "print('\\n')\n",
    "print(confusion_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disease_types=['COVID', 'non-COVID']\n",
    "Y_pred = np.argmax(y_pred)\n",
    "Y_true = np.argmax(y_test)\n",
    "\n",
    "plt.figure(figsize=(7, 7))\n",
    "ax = sns.heatmap(confusion_matrix , cmap=plt.cm.GnBu , \n",
    "                 annot=True, fmt='d', square=True, xticklabels=disease_types, \n",
    "                 yticklabels=disease_types)\n",
    "ax.set_ylabel('Actual', fontsize=20)\n",
    "ax.set_xlabel('Predicted', fontsize=20)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_history(model_history):\n",
    "   fig, axs = plt.subplots(1,2,figsize=(15,5))\n",
    "   # summarize history for accuracy\n",
    "   axs[0].plot(range(1, len(model_history['accuracy']) + 1), model_history['accuracy'])\n",
    "   axs[0].plot(range(1, len(model_history['val_accuracy']) + 1), model_history['val_accuracy'])\n",
    "   axs[0].set_title('Model Accuracy')\n",
    "   axs[0].set_ylabel('Accuracy')\n",
    "   axs[0].set_xlabel('Epoch')\n",
    "   axs[0].set_xticks(np.arange(1, len(model_history['accuracy']) + 1), len(model_history['accuracy']) / 10)\n",
    "   axs[0].legend(['train', 'val'], loc='best')\n",
    "   # summarize history for loss\n",
    "   axs[1].plot(range(1, len(model_history['loss'])+1), model_history['loss'])\n",
    "   axs[1].plot(range(1, len(model_history['val_loss']) + 1), model_history['val_loss'])\n",
    "   axs[1].set_title('Model Loss')\n",
    "   axs[1].set_ylabel('Loss')\n",
    "   axs[1].set_xlabel('Epoch')\n",
    "   axs[1].set_xticks(np.arange(1, len(model_history['loss']) + 1), len(model_history['loss']) / 10)\n",
    "   axs[1].legend(['train', 'val'], loc='best')\n",
    "   plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('models', 'xRGM1.1-Net-HistDict'), 'rb') as f:\n",
    "    history = pickle.load(f)\n",
    "    \n",
    "plot_model_history(history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
