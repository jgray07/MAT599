{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import traceback\n",
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout, Lambda, LeakyReLU, AveragePooling2D, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.pyplot import hist\n",
    "from PIL import Image\n",
    "from scipy.stats import zscore\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config & Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_WIDTH, IMAGE_HEIGHT = 227, 227\n",
    "random.seed(52)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(input_data, output_data, batch_size, is_training):\n",
    "    input_data_batch = np.empty([batch_size, IMAGE_HEIGHT, IMAGE_WIDTH, 1])\n",
    "    output_data_batch = np.empty([batch_size, 1])\n",
    "    while True:\n",
    "        index = 0\n",
    "        for i in np.random.permutation(input_data.shape[0]):\n",
    "            input_data_batch[index] = input_data[i]\n",
    "            output_data_batch[index] = output_data[i]\n",
    "            index += 1\n",
    "            if index == batch_size:\n",
    "                break\n",
    "        yield input_data_batch, output_data_batch\n",
    "\n",
    "\n",
    "def _preprocess(image_loc: str) -> np.ndarray:\n",
    "    f_image = Image.open(image_loc).convert('L')\n",
    "    f_image = f_image.resize((IMAGE_WIDTH, IMAGE_HEIGHT), Image.ANTIALIAS)\n",
    "    image = np.asarray(f_image)\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    image = np.expand_dims(image, axis=3)\n",
    "    return image\n",
    "\n",
    "\n",
    "def preprocess(loc: str) -> np.ndarray:\n",
    "    \"\"\"Safety wrapper over _preprocess.\n",
    "    \n",
    "    Args:\n",
    "        loc: File location to check if dir or an image.\n",
    "        \n",
    "    Returns:\n",
    "        Preprocessed image(s) as an ndarray.\n",
    "    \"\"\"\n",
    "    if os.path.isdir(loc):\n",
    "        current = np.concatenate((\n",
    "            [_preprocess(os.path.join(loc, image)) for image in os.listdir(loc)\n",
    "             if '.png' in image]\n",
    "        ))\n",
    "    else:\n",
    "        current = _preprocess(loc)\n",
    "    return current\n",
    "\n",
    "\n",
    "def generate_data_arrays(\n",
    "    covid_data: str = 'rsc/New_Data_CoV2/COVID/',\n",
    "    noncovid_data: str = 'rsc/New_Data_CoV2/non-COVID/',\n",
    "    others_data:  str = 'rsc/New_Data_CoV2/Others/',\n",
    "    image_output: str = 'rsc/input_data_v2',\n",
    "    label_output: str = 'rsc/output_data_v2',\n",
    "    regenerate: bool = True\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Loads COVID images into ndarrays.\n",
    "    \n",
    "    Args:\n",
    "        covid_data: Relative path to folder containing COVID images.\n",
    "        noncovid_data: Relative path to folder containing non-COVID images.\n",
    "        others_data: Relative path to folder containing Others images.\n",
    "        image_output: Relative path (with filename) to save loaded images to.\n",
    "        label_output: Relative path (with filename) to save loaded classes (labels) to.\n",
    "        regenerate: If True, will regenerate saved ndarray files for both image_output\n",
    "            and label_output, even if they already exist (overwriting them). If False,\n",
    "            will simply load and return the aforementioned.\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of ndarrays. The first is the loaded image data in one large ndarray,\n",
    "        and the second is the loaded labels, also in a large ndarray. These directly\n",
    "        match up to each other ONLY when both arrays (and their respective files)\n",
    "        are generated at the same time with this method.\n",
    "    \"\"\"\n",
    "    if (regenerate is False and \n",
    "        os.path.isfile(f'{out_input_data}.npy') and\n",
    "        os.path.isfile(f'{out_output_data}.npy')):\n",
    "        return (np.load(f'{out_input_data}.npy'),\n",
    "                np.load(f'{out_output_data}.npy'))\n",
    "    \n",
    "    if not ((cd := os.path.exists(covid_data)) and (ncd := os.path.exists(noncovid_data))):\n",
    "        if (others_data is not None) and not (odd := os.path.exists(others_data)):\n",
    "            raise NotADirectoryError(\"Directories do not exist.\")\n",
    "        else:\n",
    "            raise NotADirectoryError(\"COVID/non-COVID directories do not exist.\")\n",
    "    \n",
    "    im_data = np.empty([1, IMAGE_WIDTH, IMAGE_WIDTH, 1])\n",
    "    output = np.array([0])\n",
    "\n",
    "    print('Loading Covid Data')\n",
    "    for i in tqdm(glob.glob(os.path.join(covid_data, '**'))):\n",
    "        current = preprocess(i)\n",
    "        im_data = np.concatenate((im_data, current))\n",
    "        output = np.concatenate((output, np.array([1])), axis=0)\n",
    "\n",
    "    print('Loading non-Covid Data')\n",
    "    for i in tqdm(glob.glob(os.path.join(noncovid_data, '**'))):\n",
    "        current = preprocess(i)\n",
    "        im_data = np.concatenate((im_data, current))\n",
    "        output = np.concatenate((output, np.array([0])), axis=0)\n",
    "        \n",
    "    if others_data is not None:\n",
    "        print('Loading Others Data')\n",
    "        for i in tqdm(glob.glob(os.path.join(others_data, '**'))):\n",
    "            current = preprocess(i)\n",
    "            im_data = np.concatenate((im_data, current))\n",
    "            output = np.concatenate((output, np.array([-1])), axis=0)\n",
    "    else:\n",
    "        print('Others data not provided; skipping.')\n",
    "\n",
    "\n",
    "    print('Finished Loading Data')\n",
    "    im_data = np.delete(im_data, 0, axis=0)\n",
    "    output = np.delete(output, 0, axis=0)\n",
    "\n",
    "    zip_data = list(zip(im_data, output))\n",
    "    random.shuffle(zip_data)\n",
    "    im_data, output = zip(*zip_data)\n",
    "\n",
    "    np.save(f'{image_output}', im_data)\n",
    "    np.save(f'{label_output}', output)\n",
    "    return im_data, output\n",
    "\n",
    "\n",
    "def load_data(image_loc: str, label_loc: str, **kwargs) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Attempts to load generated images and labels.\n",
    "    \n",
    "    If either is not found, attempts to (re)generate them using the default\n",
    "    input arguments to the generation method for the raw data, but the input\n",
    "    `image_loc` and `label_loc` parameters for the output arrays. If this\n",
    "    still does not work, returns a general error and exits.\n",
    "    \n",
    "    Args:\n",
    "        image_loc: Relative path to saved image ndarray file.\n",
    "        label_loc: Relative path to saved label ndarray file.\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of ndarrays. The first array holds the loaded image data, whilst\n",
    "        the second array holds their respective labels.\n",
    "    \n",
    "    \"\"\"\n",
    "    try:\n",
    "        image_data = np.load(f'{image_loc}.npy')\n",
    "        label_data = np.load(f'{label_loc}.npy')\n",
    "        return image_data, label_data\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        try:\n",
    "            image_data, label_data = generate_data_arrays(\n",
    "                image_output = image_loc,\n",
    "                label_output = label_loc,\n",
    "                **kwargs\n",
    "            )\n",
    "            return image_data, label_data\n",
    "        \n",
    "        except Exception as e:\n",
    "            traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Version 1\n",
    "im_data_v1, output_v1 = load_data(\n",
    "    image_loc = 'rsc/input_data_v1',\n",
    "    label_loc = 'rsc/output_data_v1',\n",
    "    covid_data = 'rsc/Data_CoV2/COVID/',\n",
    "    noncovid_data = 'rsc/Data_CoV2/non-COVID/',\n",
    "    others_data = None,\n",
    ")\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(im_data_v1, output_v1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(X_Train, Y_Train, X_Test, Y_Test, model_dir: str):\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "    #X_Train, X_Test, Y_Train, Y_Test = train_test_split(input_data, output_data, test_size=0.1)\n",
    "\n",
    "    for_pad = lambda s: s if s > 2 else 3\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Lambda(lambda x: x, input_shape=(IMAGE_WIDTH, IMAGE_HEIGHT, 1)))\n",
    "\n",
    "    #model.add(ZeroPadding2D(padding=(for_pad(3) - 1)//2))\n",
    "    model.add(Conv2D(kernel_size=(3, 3), filters=8, strides=(1, 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid'))\n",
    "\n",
    "    #model.add(ZeroPadding2D(padding=(for_pad(3) - 1)//2))\n",
    "    model.add(Conv2D(kernel_size=(3, 3), filters=16, strides=(1, 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "    #model.add(ZeroPadding2D(padding=(for_pad(3) - 1)//2))\n",
    "    model.add(Conv2D(kernel_size=(3, 3), filters=32, strides=(1, 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    #model.add(ZeroPadding2D(padding=(for_pad(1) - 1)//2))\n",
    "    model.add(Conv2D(kernel_size=(1, 1), filters=16, strides=(1, 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    #model.add(ZeroPadding2D(padding=(for_pad(3) - 1)//2))\n",
    "    model.add(Conv2D(kernel_size=(3, 3), filters=32, strides=(1, 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "    #model.add(ZeroPadding2D(padding=(for_pad(3) - 1)//2))\n",
    "    model.add(Conv2D(kernel_size=(3, 3), filters=64, strides=(1, 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    #model.add(ZeroPadding2D(padding=(for_pad(1) - 1)//2))\n",
    "    model.add(Conv2D(kernel_size=(1, 1), filters=32, strides=(1, 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    #model.add(ZeroPadding2D(padding=(for_pad(3) - 1)//2))\n",
    "    model.add(Conv2D(kernel_size=(3, 3), filters=64, strides=(1, 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "\n",
    "    #model.add(ZeroPadding2D(padding=(for_pad(3) - 1)//2))\n",
    "    model.add(Conv2D(kernel_size=(3, 3), filters=128, strides=(1, 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    #model.add(ZeroPadding2D(padding=(for_pad(1) - 1)//2))\n",
    "    model.add(Conv2D(kernel_size=(1, 1), filters=64, strides=(1, 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    #model.add(ZeroPadding2D(padding=(for_pad(3) - 1)//2))\n",
    "    model.add(Conv2D(kernel_size=(3, 3), filters=128, strides=(1, 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid'))\n",
    "\n",
    "\n",
    "    #model.add(ZeroPadding2D(padding=(for_pad(3) - 1)//2))\n",
    "    model.add(Conv2D(kernel_size=(3, 3), filters=256, strides=(1, 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    #model.add(ZeroPadding2D(padding=(for_pad(1) - 1)//2))\n",
    "    model.add(Conv2D(kernel_size=(1, 1), filters=128, strides=(1, 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "    model.add(AveragePooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid'))\n",
    "\n",
    "    #model.add(ZeroPadding2D(padding=(for_pad(3) - 1)//2))\n",
    "    model.add(Conv2D(kernel_size=(3, 3), filters=256, strides=(1, 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "    model.add(AveragePooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid'))\n",
    "    #model.add(ZeroPadding2D(padding=(for_pad(1) - 1)//2))\n",
    "    model.add(Conv2D(kernel_size=(1, 1), filters=128, strides=(1, 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "\n",
    "\n",
    "    #model.add(ZeroPadding2D(padding=(for_pad(19) - 1)//2))\n",
    "    model.add(Conv2D(kernel_size=(3, 3), filters=256, strides=(1, 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "    #model.add(Conv2D(kernel_size=(3, 3), filters=2, strides=(1, 1),activation='relu'))\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dropout(rate=0.4))\n",
    "    model.add(Dense(units=338, activation='relu'))\n",
    "    model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(optimizer=Adam(lr=3e-4), loss='binary_crossentropy', metrics=[\"accuracy\"])\n",
    "    model.summary()\n",
    "\n",
    "    checkpoint = ModelCheckpoint(os.path.join(model_dir, 'xRGM1.2'), verbose=1, period=1, save_best_only=True)\n",
    "\n",
    "    hist = model.fit_generator(batch_generator(X_Train, Y_Train, 128, True),\n",
    "                        validation_data=(batch_generator(X_Test, Y_Test, 128, False)), callbacks=[checkpoint],\n",
    "                        verbose=1, epochs=10, steps_per_epoch=1000, validation_steps=len(X_Test))\n",
    "\n",
    "    with open(os.path.join('models', 'xRGM1.2-Net-HistDict'), 'wb') as f:\n",
    "        pickle.dump(hist.history, f)\n",
    "\n",
    "    model.save(model_dir)\n",
    "    \n",
    "    return model\n",
    "#changed batchsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lambda_1 (Lambda)            (None, 227, 227, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 225, 225, 8)       80        \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 225, 225, 8)       32        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)   (None, 225, 225, 8)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 112, 112, 8)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 110, 110, 16)      1168      \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 110, 110, 16)      64        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)   (None, 110, 110, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 108, 108, 32)      4640      \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 108, 108, 32)      128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_18 (LeakyReLU)   (None, 108, 108, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 108, 108, 16)      528       \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 108, 108, 16)      64        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_19 (LeakyReLU)   (None, 108, 108, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 106, 106, 32)      4640      \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 106, 106, 32)      128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_20 (LeakyReLU)   (None, 106, 106, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 104, 104, 64)      18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 104, 104, 64)      256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_21 (LeakyReLU)   (None, 104, 104, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 104, 104, 32)      2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 104, 104, 32)      128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_22 (LeakyReLU)   (None, 104, 104, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 102, 102, 64)      18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 102, 102, 64)      256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_23 (LeakyReLU)   (None, 102, 102, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 100, 100, 128)     73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_24 (Batc (None, 100, 100, 128)     512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_24 (LeakyReLU)   (None, 100, 100, 128)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 100, 100, 64)      8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_25 (Batc (None, 100, 100, 64)      256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_25 (LeakyReLU)   (None, 100, 100, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 98, 98, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_26 (Batc (None, 98, 98, 128)       512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_26 (LeakyReLU)   (None, 98, 98, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 49, 49, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_27 (Conv2D)           (None, 47, 47, 256)       295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_27 (Batc (None, 47, 47, 256)       1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_27 (LeakyReLU)   (None, 47, 47, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_28 (Conv2D)           (None, 47, 47, 128)       32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_28 (Batc (None, 47, 47, 128)       512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_28 (LeakyReLU)   (None, 47, 47, 128)       0         \n",
      "_________________________________________________________________\n",
      "average_pooling2d_2 (Average (None, 23, 23, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_29 (Conv2D)           (None, 21, 21, 256)       295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_29 (Batc (None, 21, 21, 256)       1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_29 (LeakyReLU)   (None, 21, 21, 256)       0         \n",
      "_________________________________________________________________\n",
      "average_pooling2d_3 (Average (None, 10, 10, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_30 (Conv2D)           (None, 10, 10, 128)       32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_30 (Batc (None, 10, 10, 128)       512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_30 (LeakyReLU)   (None, 10, 10, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_31 (Conv2D)           (None, 8, 8, 256)         295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_31 (Batc (None, 8, 8, 256)         1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_31 (LeakyReLU)   (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 16384)             0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16384)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 338)               5538130   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 339       \n",
      "=================================================================\n",
      "Total params: 6,702,293\n",
      "Trainable params: 6,699,077\n",
      "Non-trainable params: 3,216\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jg1994/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 495/1000 [=============>................] - ETA: 14:46:29 - loss: 0.2230 - accuracy: 0.9161"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_dir: str = 'model/xRGM1.2'\n",
    "xRGM = make_model(x_train, y_train.reshape(-1,1), x_test, y_test.reshape(-1,1), model_dir=model_dir)\n",
    "\n",
    "v_classify = np.vectorize(lambda x: np.round(x, 0))\n",
    "# xRGMCT2 = load_model('models/xRGMCT2L.h5')\n",
    "# print(xRGMCT2.history)\n",
    "y_pred = (xRGM.predict(x_test))\n",
    "\n",
    "#print(y_test)\n",
    "#print(y_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = pd.DataFrame(metrics.confusion_matrix(y_test, v_classify(y_pred)),\n",
    "                                columns=['Predicted Negative', 'Predicted Positive'],\n",
    "                                index=['Actual Negative', 'Actual Positive'])\n",
    "\n",
    "tp, tn, fp, fn = (\n",
    "    confusion_matrix.iloc[1,1], \n",
    "    confusion_matrix.iloc[0,0], \n",
    "    confusion_matrix.iloc[0,1], \n",
    "    confusion_matrix.iloc[1,0]\n",
    ")\n",
    "\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "sensitivity = tp / (tp + fn)\n",
    "specificity = tn / (tn + fp)\n",
    "f1_score = 2 * ((specificity * sensitivity) / (specificity + sensitivity))\n",
    "\n",
    "with open(os.path.join(f'{model_dir}', 'metrics.csv'), 'a', encoding='utf-8') as f:\n",
    "    f.write(\n",
    "        f'{time.strftime(\"%Y-%m-%dT%H:%M:%S-05:00\", time.localtime())}, {accuracy}, {sensitivity}, {specificity}, {f1_score}\\n'\n",
    "    )\n",
    "\n",
    "print(f'Accuracy:    {accuracy}')\n",
    "print(f'Sensitivity: {sensitivity}')\n",
    "print(f'Specificity: {specificity}')\n",
    "print(f'F1 Score:    {f1_score}')\n",
    "print('\\n')\n",
    "print(confusion_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disease_types=['COVID', 'non-COVID']\n",
    "Y_pred = np.argmax(y_pred)\n",
    "Y_true = np.argmax(y_test)\n",
    "\n",
    "plt.figure(figsize=(7, 7))\n",
    "ax = sns.heatmap(confusion_matrix , cmap=plt.cm.GnBu , \n",
    "                 annot=True,fmt='d', square=True, xticklabels=disease_types, \n",
    "                 yticklabels=disease_types)\n",
    "ax.set_ylabel('Actual', fontsize=20)\n",
    "ax.set_xlabel('Predicted', fontsize=20)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_history(model_history):\n",
    "   fig, axs = plt.subplots(1,2,figsize=(15,5))\n",
    "   # summarize history for accuracy\n",
    "   axs[0].plot(range(1, len(model_history['accuracy']) + 1), model_history['accuracy'])\n",
    "   axs[0].plot(range(1, len(model_history['val_accuracy']) + 1), model_history['val_accuracy'])\n",
    "   axs[0].set_title('Model Accuracy')\n",
    "   axs[0].set_ylabel('Accuracy')\n",
    "   axs[0].set_xlabel('Epoch')\n",
    "   axs[0].set_xticks(np.arange(1, len(model_history['accuracy']) + 1), len(model_history['accuracy']) / 10)\n",
    "   axs[0].legend(['train', 'val'], loc='best')\n",
    "   # summarize history for loss\n",
    "   axs[1].plot(range(1, len(model_history['loss'])+1), model_history['loss'])\n",
    "   axs[1].plot(range(1, len(model_history['val_loss']) + 1), model_history['val_loss'])\n",
    "   axs[1].set_title('Model Loss')\n",
    "   axs[1].set_ylabel('Loss')\n",
    "   axs[1].set_xlabel('Epoch')\n",
    "   axs[1].set_xticks(np.arange(1, len(model_history['loss']) + 1), len(model_history['loss']) / 10)\n",
    "   axs[1].legend(['train', 'val'], loc='best')\n",
    "   plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('models', 'xRGM1.2-Net-HistDict'), 'rb') as f:\n",
    "    history = pickle.load(f)\n",
    "    \n",
    "plot_model_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Version 2\n",
    "im_data_v2, output_v2 = generate_data_arrays()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
